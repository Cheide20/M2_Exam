{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAyiccILvYWI"
      },
      "source": [
        "# SDS 2. Exam assignment\n",
        "In this assignment we have to produce an analysis using a combination of Natural Language Processing (NLP) and Network Analysis techniques. Our analysis will try and look upon horror movie reviews in the IMDB dataset.\n",
        "\n",
        "A logical hypothesis could thus be around halloween given it just have parsed and often is a subject in horror movies. This just ended up taking to much time to run, therefore a known horror movie was selected in 'A Nightmare on Elm Street':\n",
        "\n",
        "***What is most central in reviews regarding the 'A Nightmare on Elm Street' movie and what is the sentiment regarding this movie?***\n",
        "\n",
        "We are given 3 topics and some sub-elements that we need to answer while solving for this hypothesis This being:\n",
        "\n",
        "\n",
        "*   Data processing with LLM's\n",
        "*   Network analysis\n",
        "*   Text classification\n",
        "\n",
        "Each part is given a section below in this jupyter notebook.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jWpkLhJxUVj"
      },
      "source": [
        "# 1. Data Processing with LLMs\n",
        "We have to do the foolowing steps:\n",
        "\n",
        "- Implement local or cloud-based Large Language Models (LLMs) to:\n",
        "  - Extract and structure relevant market data\n",
        "  - Identify network relationships between entities\n",
        "  - Perform named entity recognition and extraction\n",
        "  - Transform unstructured text into analyzable formats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Block 1**\n",
        "\n",
        "  The first lines of code below is used to install necessary packages into collab. Transformers is used for languagemoddels, datasets to import datasets, pandas for datamanipulation, matplotlib for visualization and so on."
      ],
      "metadata": {
        "id": "z98hqiQ6h3Bk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ewgNkEKy2nO",
        "outputId": "a4ac30f0-8fa8-404e-dff1-8fd46cb73d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m471.0/472.7 kB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# We start by installing necessary dependencies:\n",
        "!pip install transformers datasets pandas spacy ollama networkx matplotlib tqdm -q\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PoC9Kbd3EH4"
      },
      "source": [
        "**Code block 2 and 3**\n",
        "\n",
        "The next two codeblocks are used to create a connection to ollama which makes it possible to promt LLM to extract structured marketdata.\n",
        "\n",
        "The code just below is used to install ollama, where the code below that is used to start the ollama server, which is used for the communication with the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xKDvIAIJzLI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33699145-79f1-464c-aa60-b4da76e8bdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 2s (199 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "#########                                                                                      10.4%"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnvIJ68fzOKd"
      },
      "outputs": [],
      "source": [
        "# Required packages\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import json\n",
        "import ollama\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "def start_ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YHiCKyj3_PK"
      },
      "source": [
        "**Code block 4**\n",
        "\n",
        "This is the LLM that we chose to use since it was the best at the moment given its various scores explained in the lectures. The model will as described above be used to extract marketdata and help with text manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fDTAutY0zT2y"
      },
      "outputs": [],
      "source": [
        "!ollama pull qwen2.5:14b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0bgTHOt5lUx"
      },
      "source": [
        "**Code block 5**\n",
        "\n",
        "This code firstly import the package for downloading datasets and the imports it. Given its the IMDB dataset we have decided to work with this is the one downloaded. The dataset comes from Huggingface and have reviews on different movies. After importing it we look upon the data and see that we have a training set, test set and unsupervised with everything in it. We can therefor if we want to work with upto 50000 reviews. This will though require a lot of processing power to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fwx7yqSTxGwM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"imdb\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee6_bBw0mQu9"
      },
      "source": [
        "**Code block 6**\n",
        "\n",
        "This code first selects the training set and later shows the output. Since we in the assignment want an analyzable format this format neets to be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP2QCbXXyAp8"
      },
      "outputs": [],
      "source": [
        "train_data = data['train']\n",
        "train_data[[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drr3ZFOi7dA8"
      },
      "source": [
        "**Code block 7**\n",
        "\n",
        "The dataset is transformed into a dataframe using the pandas package, to be able to easy filter the dataset. We could filter it based on movie titles or synonyms often used in horror movies, this would just give us a big dataset and require a lot of time when prompting the LLM. Therefore a movie is selected to minimize the time spent waiting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H57nIuGqlBF"
      },
      "outputs": [],
      "source": [
        "train_data = pd.DataFrame(train_data)\n",
        "#horror_movies = ['The Exorcist', 'Psycho', 'Halloween', 'A Nightmare on Elm Street', 'The Shining', 'Scream']\n",
        "horror_movies = ['A Nightmare on Elm Street']\n",
        "horror_reviews = train_data[train_data['text'].str.contains('|'.join(horror_movies), case=False, na=False)]\n",
        "print(horror_reviews.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh9WnX6z8ZaD"
      },
      "source": [
        "**Code block 8**\n",
        "\n",
        "This gives us a dataset for horror_reviews of 34. Eg 34 reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXoL_C6XyhYn"
      },
      "outputs": [],
      "source": [
        "horror_reviews.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFFi7MSDxkCm"
      },
      "source": [
        "**Code block 9**\n",
        "\n",
        "This code is used to test the connection to the LLM by gennerating a prompt and showing the result. It can here be seen that there seems to be a valid connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NXHDtM9EXNv"
      },
      "outputs": [],
      "source": [
        "response = ollama.generate(model=\"qwen2.5:14b\", prompt=\"Extract market-related information from the text: example text\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCUyn8hr9gfH"
      },
      "source": [
        "**Code block 10**\n",
        "\n",
        "Here we create a function that allows us to extract the market related information from each review. The data is then returned to the defined structured_text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unyF6ijOxn5m"
      },
      "outputs": [],
      "source": [
        "def extract_market_data(text):\n",
        "    response = ollama.generate(model=\"qwen2.5:14b\", prompt=f\"Extract market-related information from the text: {text}\")\n",
        "\n",
        "    if 'response' in response:\n",
        "        structured_text = response['response']\n",
        "        return structured_text\n",
        "    else:\n",
        "        return {\"error\": \"Unexpected response format\", \"response\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO5gYtJc-NiA"
      },
      "source": [
        "**Code block 11**\n",
        "\n",
        "This code is used to create an example to test whether our code works. Eg that we get the response as a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz8qzCQTCswD"
      },
      "outputs": [],
      "source": [
        "sample_text = horror_reviews['text'].iloc[0]\n",
        "print(extract_market_data(sample_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cFZGJpB-oPD"
      },
      "source": [
        "**Code block 12**\n",
        "\n",
        "Since the format should be structured and easy to work with. The json format is chosen. We then get the output which shows what rows that include the horror movie, aswell as the review text and the structured_json format that is returned by the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubL_dyOGExhi"
      },
      "outputs": [],
      "source": [
        "horror_reviews['structured_json'] = horror_reviews['text'].apply(extract_market_data)\n",
        "print(horror_reviews[['text', 'structured_json']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN1-BIdr_TX-"
      },
      "source": [
        "**Code block 13**\n",
        "\n",
        "This code uses the spacy package to find named entities, such as persons or places from the reviews and saves it in a new coloumn named entities_json. This is done to later allow us to make network analysis on the data since places and persons can be connected to eachother."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_yQ19_gE16d"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "horror_reviews['entities_json'] = horror_reviews['text'].apply(extract_entities)\n",
        "print(horror_reviews[['text', 'entities_json']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ib-v998xb5O"
      },
      "source": [
        "# 2. Network Analysis\n",
        "\n",
        "- Design and construct meaningful networks from the extracted data\n",
        "- Implement bi-partite network analysis and corresponding projections\n",
        "- Calculate and interpret key network metrics:\n",
        "  - Various centrality measures\n",
        "  - Network structure indicators\n",
        "  - Community detection (if applicable)\n",
        "- Provide clear interpretation of network analysis results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 14**\n",
        "\n",
        "This code constructs a network graph using entities from a dataset of horror reviews, calculates centrality measures, and identifies the top entities based on their centrality."
      ],
      "metadata": {
        "id": "FbadwJykFwUZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFijfMUmxoe9"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "for _, row in horror_reviews.iterrows():\n",
        "    entities = row['entities_json']\n",
        "    for i, entity1 in enumerate(entities):\n",
        "        for j, entity2 in enumerate(entities):\n",
        "            if i < j:\n",
        "                G.add_edge(entity1[0], entity2[0])\n",
        "\n",
        "centrality = nx.degree_centrality(G)\n",
        "print(\"Top entities by centrality:\", sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entities with greatest importance for centrality is \"Elm Street\", \"two\" and \"Nightmare\", these keywords represtent connected titles in the network and as expected are connected to the title of the movie. Futhermore \"Wes Craven's\" has a high centrality which makes sence since the film are directed/produced by him.\n",
        "\n",
        "The scores shows there is not a extreme influence of one keyword, but more spread influence."
      ],
      "metadata": {
        "id": "cKBQW56dpCZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 15**\n",
        "\n",
        "This code creates a bipartite graph that connects the horror genre to associated entities (like characters, locations, concepts or other) extracted from a collection of horror reviews. Each horror node is linked to the various entities mentioned within the reviews. After building this network, the code then generates a projection focused solely on the enitities. In this projection, entities er connected if they occur more than once in the same review, with edge wheights indicating the frequency of this. Therefore it allows us to analyse entity relationships within the horrer genre based on prominent entities and their connections."
      ],
      "metadata": {
        "id": "hAdsaaHNGFvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU2jbzVJV34f"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "from collections import defaultdict\n",
        "B = nx.Graph()\n",
        "genre_node = \"Horror\"\n",
        "B.add_node(genre_node, bipartite=0)\n",
        "co_occurrence_counts = defaultdict(int)\n",
        "for _, row in horror_reviews.iterrows():\n",
        "    entities = row['entities_json']\n",
        "    unique_entities = set(entities)\n",
        "    for entity in unique_entities:\n",
        "        B.add_node(entity, bipartite=1)\n",
        "        B.add_edge(genre_node, entity)\n",
        "    for entity1 in unique_entities:\n",
        "        for entity2 in unique_entities:\n",
        "            if entity1 != entity2:\n",
        "                co_occurrence_counts[(entity1, entity2)] += 1\n",
        "entity_projection = nx.Graph()\n",
        "for (entity1, entity2), count in co_occurrence_counts.items():\n",
        "    if count > 1:\n",
        "        entity_projection.add_edge(entity1, entity2, weight=count)\n",
        "print(\"Number of nodes in entity projection:\", entity_projection.number_of_nodes())\n",
        "print(\"Number of edges in entity projection:\", entity_projection.number_of_edges())\n",
        "weighted_centrality = nx.degree_centrality(entity_projection)\n",
        "print(\"Top entities by weighted centrality:\", sorted(weighted_centrality.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics of nodes and edges show a relative dense network with multiple connections between entities. This is done to help identify central themes within the horror narative in the movie.\n",
        "\n",
        "- **Number of nodes: 34** – This indicates that the total number of unique entities identified in the reviews are 34. Each node represents a specific entity associated with the horror genre through the reviews of \"A Nightmare on Elm Street.\"\n",
        "- **Number of edges: 102** – The number of edges is the count of connections between different entities. An edge between two entities signifies a co-occurence in the same review.  \n",
        "- **Top entities** – Lastly the top entities based on weighted centrality is found. Here it can be notessed that Elm street, the producer and persons in general is very central for the reviews."
      ],
      "metadata": {
        "id": "DTug0WAXpeXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 16**\n",
        "\n",
        "This code analyzes which entities play a key role in the network."
      ],
      "metadata": {
        "id": "T9clE30iG8Mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSaLU1TQV30a"
      },
      "outputs": [],
      "source": [
        "degree_centrality = nx.degree_centrality(entity_projection)\n",
        "print(\"Top entities by degree centrality:\", sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10])\n",
        "betweenness_centrality = nx.betweenness_centrality(entity_projection)\n",
        "print(\"Top entities by betweenness centrality:\", sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10])\n",
        "eigenvector_centrality = nx.eigenvector_centrality(entity_projection)\n",
        "print(\"Top entities by eigenvector centrality:\", sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen in the results that even though different centrality measueres is used there are very little change in the key entities. The differences in the results are caused by the differences in the methods. The methods used is:\n",
        "\n",
        "Degree Centrality\n",
        "- Shows the most direct connections to other entities.\n",
        "\n",
        "Betweenness Centrality\n",
        "- Shows which entities help as connections to other entities another way of expressing the relationship is that its the entities that lies between other entities the most.\n",
        "\n",
        "Eigenvector Centrality\n",
        "- Shows the influence of entities within the network as the most central eg. two, wes craven and elm street often is connected to other central entities."
      ],
      "metadata": {
        "id": "8L34VR2urW4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 17**\n",
        "\n",
        "\n",
        "This code examines the clustering and connectivity of the entities. By analyzing the clustering coefficient and average path length, it gains insights into how closely related the entities is to eachther in the network."
      ],
      "metadata": {
        "id": "ZXEiXBA5HMgQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqDBWHX9IZM_"
      },
      "outputs": [],
      "source": [
        "clustering_coefficient = nx.average_clustering(entity_projection)\n",
        "print(\"Clustering Coefficient:\", clustering_coefficient)\n",
        "if nx.is_connected(entity_projection):\n",
        "    avg_path_length = nx.average_shortest_path_length(entity_projection)\n",
        "    print(\"Average Path Length:\", avg_path_length)\n",
        "else:\n",
        "    print(\"The graph is not connected, so the average path length cannot be calculated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering coefficient of 0.658 indicates that approximet 66% of entities connected to a common entity such as character or location also is interconnected. This indicates that only 33% of entities in the reviews of the movie isnt frequently occuring.\n",
        "\n",
        "The average path length of 2.21 indicates a short average path length. What this means is that the netwrok is highly interconnected. This makes sence since its only one movie thats looked upon and the focus of the reviews is the main story rather than the small details of none important entities."
      ],
      "metadata": {
        "id": "VoEadfhctGrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 18**\n",
        "\n",
        "This code creates an interactive and visually dynamic representation of a network graph, showcasing how entities are interconnected. Leveraging Plotly for interactivity and NetworkX for graph structure, it provides an engaging plot where each entity is a node, and connections are formed based on shared associations. The result is a scalable visualization that highlights each entities role and prominence within the network.\n",
        "\n",
        "By calculating and representing centrality, the plot identifies frequently occuring entities, potentially highlighting recurring themes or influential carracter within the horror narative of the movie. Users can interact with the graph by hovering over nodes to see individual titles and centrality scores, offering a clear, user-friendly way to explore the web of relationships. The scalable nature of the visualization allows for a high-level overview of connectivity, while also enabling detailed examination of specific entities and their connections within the network."
      ],
      "metadata": {
        "id": "CgELeFG-HXPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKwRW1EWI2Ut"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "pos = nx.spring_layout(entity_projection, k=0.3, seed=42)\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "for edge in entity_projection.edges():\n",
        "    x0, y0 = pos[edge[0]]\n",
        "    x1, y1 = pos[edge[1]]\n",
        "    edge_x += [x0, x1, None]\n",
        "    edge_y += [y0, y1, None]\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_sizes = []\n",
        "node_text = []\n",
        "\n",
        "for node in entity_projection.nodes():\n",
        "    x, y = pos[node]\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_sizes.append(10 + 50 * degree_centrality[node])  # scale node sizes\n",
        "    node_text.append(f\"{node}<br>Degree Centrality: {degree_centrality[node]:.2f}\")\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition=\"top center\",\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        showscale=True,\n",
        "        colorscale='Blues',\n",
        "        size=node_sizes,\n",
        "        color=node_sizes,\n",
        "        colorbar=dict(\n",
        "            title=\"Degree Centrality\",\n",
        "            thickness=15,\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line_width=2))\n",
        "\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title=\"Interactive Projection Network\",\n",
        "                    title_x=0.5,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=0, l=0, r=0, t=40),\n",
        "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                )\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network shows several prominent entities such as Elm Street, Wes Craven and so on which is also shown above.\n",
        "\n",
        "Each node’s size and color intensity represent its degree centrality, showing the number of direct connections (shared entities or themes) it has with other nodes. Nodes with higher degree centrality are larger and darker, indicating they have more connections or edges in the network.\n",
        "\n",
        "There is pramarily two clusters in the network that is clusterd around either Elm street or the persons Rachel McAdams or Wes Craven. It is also clear that there is a distinction of persons and what we can assume is whats happening or regarding elm street."
      ],
      "metadata": {
        "id": "0DX8fpBjtrKL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV51nf3cxo3S"
      },
      "source": [
        "# 3. Text Classification Select and implement one of these approaches:\n",
        "\n",
        "- LLM-based classification system\n",
        "- **Few-shot learning implementation using SetFit**\n",
        "- Traditional NLP classification methods (using existing or synthetic training data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkeXKJmP_O3e"
      },
      "source": [
        "Classifies the reviews into: Positive, negative or neutral"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 19**\n",
        "\n",
        "This code prepares to implement SetFit, a library for fine-tuning sentence transformers for classification tasks, ideal for applications where labeled data is limited. By leveraging SetFit’s model and trainer classes, this setup facilitates efficient text classification. The code also includes steps to split the data into training and test sets using train_test_split from scikit-learn."
      ],
      "metadata": {
        "id": "ruwY3IaqI4ls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pVJd0JgU-MeE"
      },
      "outputs": [],
      "source": [
        "!pip install setfit\n",
        "\n",
        "from setfit import SetFitModel, SetFitTrainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 20**\n",
        "\n",
        "This code creates a small labeled dataset for few-shot training, providing examples of different sentiments (positive, negative, and neutral) in movie reviews. This data is then structured in a DataFrame, with sentiment labels encoded as integers, setting it up for training a classification model with limited samples."
      ],
      "metadata": {
        "id": "Sl-PRxtZJBP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpSQH_83_fA3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "few_shot_data = [\n",
        "    {\"text\": \"A terrifying and thrilling movie that kept me on the edge!\", \"label\": \"positive\"},\n",
        "    {\"text\": \"The plot was boring and uninspiring.\", \"label\": \"negative\"},\n",
        "    {\"text\": \"It was okay, not too scary but had some good moments.\", \"label\": \"neutral\"}\n",
        "]\n",
        "few_shot_df = pd.DataFrame(few_shot_data)\n",
        "few_shot_df['label'] = few_shot_df['label'].map({\"positive\": 1, \"negative\": 0, \"neutral\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 21**\n",
        "\n",
        "This code fine-tunes a sentence transformer model on a small, labeled dataset for sentiment classification using SetFit. It leverages Hugging Face’s Dataset class and the SetFitTrainer to train the model on few-shot data, making it adaptable for cases with limited labeled examples."
      ],
      "metadata": {
        "id": "xToLjDllJH13"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1XgTA_I_iCX"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from setfit import SetFitModel, SetFitTrainer\n",
        "few_shot_dataset = Dataset.from_pandas(few_shot_df)\n",
        "model = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    train_dataset=few_shot_dataset,\n",
        "    num_iterations=20\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# c671e11fe2b645508e7d3cd9c16d0177525f93cf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 22**\n",
        "\n",
        "The code block uses a sentiment analysis model to predict the sentiment of three sample horror movie reviews."
      ],
      "metadata": {
        "id": "jA1gukj6OeME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_reviews = [\"An absolute horror masterpiece!\", \"The film was a letdown.\", \"Not scary, but it was entertaining.\"]\n",
        "predictions = model(new_reviews)\n",
        "for review, sentiment in zip(new_reviews, predictions):\n",
        "    print(f\"Review: {review} -> Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "yy16dXIIMSWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i99TaEcGWQ5V"
      },
      "source": [
        "# 4. Topic Modeling\n",
        "\n",
        "1. Leverage LLMs to extract and categorize key themes and topics\n",
        "\n",
        "2. Apply BERTopic for advanced topic modeling\n",
        "\n",
        "3. Create clear and insightful visualizations of:\n",
        "  * Topic distributions\n",
        "  * Theme relationships\n",
        "  * Temporal patterns (if applicable)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 23**\n",
        "\n",
        "This code prepares the environment and imports necessary libraries for BERTopic—a topic modeling technique that uses transformer embeddings to identify themes within a text dataset. BERTopic leverages dimensionality reduction (UMAP), clustering, and transformer-based embeddings to create interpretable topic clusters, making it effective for organizing large text datasets by themes or topics."
      ],
      "metadata": {
        "id": "H_q6HhJdJqIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y75mli9vWQv6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install bertopic\n",
        "!pip install umap-learn\n",
        "!pip install sentence-transformers\n",
        "!pip install networkx\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from umap import UMAP\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 24**\n",
        "\n",
        "This line initializes a SentenceTransformer model called 'all-MiniLM-L6-v2', which is specifically designed for efficient text embedding. The model translates each sentence into a numerical embedding that captures its semantic meaning, making it suitable for various NLP tasks like clustering, topic modeling, and similarity analysis."
      ],
      "metadata": {
        "id": "Jgf9NtIQJyHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "tT1AkblhfGU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 25**\n",
        "\n",
        "This code cleans the reviews of tags and whitespace, then sets a collection of words that is excluded from the creation of topic words. From there a vectorization is made to create uni- and bigrams (1 and 2 word combinations) to enable conversion of text into a format suitable for analysis by enhances feature extraction.\n",
        "\n",
        "UMAP reduces the dimentions to 2 for plotting and with embedding the numerical representations is made. With BERTopic, the 3 previous parts is used to extract topics based on a minimum of reviews to create them."
      ],
      "metadata": {
        "id": "l0it1pEos3Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'\\s+', ' ', text)   # Remove extra whitespace\n",
        "    return text.strip()\n",
        "reviews = [preprocess_text(review) for review in horror_reviews['text'].dropna().tolist()]\n",
        "custom_stop_words = [\n",
        "    \"the\", \"of\", \"and\", \"in\", \"to\", \"that\", \"has\", \"is\", \"are\", \"one\",\n",
        "    \"it\", \"this\", \"with\", \"for\", \"as\", \"on\", \"by\", \"an\", \"at\", \"from\",\n",
        "    \"but\", \"be\", \"br\", \"about\", \"more\", \"film\", \"she\", \"he\", \"they\",\n",
        "    \"movie\", \"movies\", \"you\", \"not\", \"all\", \"was\", \"so\", \"his\"\n",
        "]\n",
        "vectorizer = CountVectorizer(stop_words=custom_stop_words, ngram_range=(1, 2))\n",
        "umap_model = UMAP(n_neighbors=15, n_components=2, min_dist=0.1, metric='cosine')\n",
        "\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer,\n",
        "    umap_model=umap_model,\n",
        "    min_topic_size=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probabilities = topic_model.fit_transform(reviews)\n"
      ],
      "metadata": {
        "id": "2-Q9p8EFpZwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 26**\n",
        "\n",
        "This code will retrive the model from the previous code block and will print the results of the gathered topics"
      ],
      "metadata": {
        "id": "gJDZS6ies1cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = topic_model.get_topic_info()\n",
        "print(topic_info)"
      ],
      "metadata": {
        "id": "hAQ6DALspfNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the topic_info table**, each topic is assigned a unique identifier, with four main topics numbered 0 through 3. An additional topic labeled -1 represents \"outliers,\" or reviews that do not strongly align with any specific topic. The \"Count\" column shows how many reviews fall into each topic; for instance, the outlier topic (-1) includes 7 reviews, while topic 1 contains 6 reviews. Each topic is briefly summarized under \"Name\" using a few prominent words, providing a quick way to identify its main themes or associated entities. For example, Topic 0 is summarized with words like \"freddy,\" \"there,\" and \"nightmare,\" suggesting a connection to the Nightmare on Elm Street series.\n",
        "\n",
        "**The \"Representation\" section** lists key terms associated with each topic, offering a more detailed view of what each topic represents. These terms are the most characteristic for each topic and help define its theme. For example, words like \"freddy,\" \"elm street,\" and \"nightmare\" in Topic 0 clearly indicate themes related to horror, particularly A Nightmare on Elm Street.\n",
        "\n",
        "**The \"Representative_Docs\" section** provides sample reviews that represent each topic, giving a better understanding of the content within each category."
      ],
      "metadata": {
        "id": "QbmGWfy7OnNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 27**\n",
        "\n",
        "The code below generates a heatmap of the produced topics and shows how similar the topics are to eachother"
      ],
      "metadata": {
        "id": "ZRIIZatts3vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_heatmap().show()"
      ],
      "metadata": {
        "id": "RYWQdv5BpqOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the similarity matrix there is shown that all topics have a higher similarity score that shows they are similar in some capacity to eachother. This means there will be similar content, language or themes within the topics\n",
        "\n",
        "The similarity matrix visualizes the relationships between two identified topics based on shared themes or terms, indicating how closely these topics are related to each other.\n",
        "\n",
        "The color gradient reflects the level of similarity, with darker blue representing higher similarity (closer to 1) and lighter green representing lower similarity (closer to 0.75). Cells along the diagonal, where each topic is compared with itself, display the darkest color, indicating perfect similarity with a score of 1.\n",
        "\n",
        "For the off-diagonal cells, which show the similarity between different topics, the comparison between Topic 0 and Topic 1 reveals moderate similarity, as shown by a light green shade, suggesting some thematic overlap. This similarity analysis helps illustrate how closely related each topic is in terms of shared elements, highlighting thematic clusters within the dataset."
      ],
      "metadata": {
        "id": "HRqpBqtJVRFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 28**\n",
        "\n",
        "The code organizes topics into a tree-like structure, where similar topics are grouped closer together, showing overarching themes and subtopics."
      ],
      "metadata": {
        "id": "ajbPI-Y3s4MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_hierarchy().show()"
      ],
      "metadata": {
        "id": "3SgETo1PphNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see the Hierarchy of the topics, where the topics has highest similarity is paired first. The graph shows a high level of similarity, but since there isnt any other topics there is not more that can be taken form this graph and no comparison between multiple topics can be made"
      ],
      "metadata": {
        "id": "5Croo0jzbGS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 29**\n",
        "\n",
        "This code block performs a manual 2D visualization of topic embeddings using PCA, this is done by reducing the dimensions and therefore simplifing the data.\n",
        "Afterwards it is plotted to show how the topic lie compared to eachother"
      ],
      "metadata": {
        "id": "ZD6okcDls4p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_embeddings = topic_model.topic_embeddings_\n",
        "reduced_embeddings = PCA(n_components=2).fit_transform(topic_embeddings)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=100)\n",
        "for i, topic in enumerate(topic_info['Name']):\n",
        "    plt.annotate(topic, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=12)\n",
        "plt.title(\"Manual 2D Visualization of Topics\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yyNYRKdWpj5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the 2D PCA visualization and the spread of the topics.\n",
        "\n",
        "The distance between points represents how similar or different the topics are. \"0_freddy_there_some_nightmare\" is closer to \"-1_her_nightmare_nightmare elm_elm street\" than to \"1_her_would_have_craven,\" indicating that these two samples may share more content. The distance shows that there is distinction between the different points."
      ],
      "metadata": {
        "id": "8oQuDfmgfd51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block 30**\n",
        "\n",
        "This code block constructs and visualizes a network graph that represents relationships among topics based on their cosine similarity and provides a visual representation of how topics are interconnected based on their similarities, helping to identify clusters and relationships within the topic space."
      ],
      "metadata": {
        "id": "JV4Rh9kSs5SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix = cosine_similarity(topic_embeddings)\n",
        "threshold = 0.6\n",
        "G = nx.Graph()\n",
        "\n",
        "for topic_num, topic_name in enumerate(topic_info['Name']):\n",
        "    if topic_num != -1:\n",
        "        G.add_node(topic_num, label=topic_name)\n",
        "\n",
        "for i in range(len(similarity_matrix)):\n",
        "    for j in range(i + 1, len(similarity_matrix[i])):\n",
        "        if similarity_matrix[i][j] > threshold:\n",
        "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
        "\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "plt.figure(figsize=(10, 10))\n",
        "nx.draw_networkx(G, pos, with_labels=True, font_size=10, node_size=3000, node_color=\"skyblue\")\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels={(i, j): f\"{similarity_matrix[i][j]:.2f}\" for i, j in G.edges})\n",
        "plt.title(\"Topic Relationship Network\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cHEjqPgOpoK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the simplification of the previous graph, where the distances between the different points is displayed numericly.\n",
        "\n",
        "The \"Topic Relationship Network\" graph illustrates the similarity between four identified topics (labeled 0, 1 and 2) based on cosine similarity scores. Each node represents a topic, with edges connecting them to show the degree of similarity, and the scores labeled on each edge.\n",
        "\n",
        "The edges' thickness and length reflect the strength of similarity. Higher scores, like the 0.84 between topics 0 and 2, indicate a stronger thematic link, suggesting these topics have more shared elements or themes. In contrast, lower scores, such as the 0.74 between topics 1 and 2, imply a weaker relationship, meaning these topics are more distinct."
      ],
      "metadata": {
        "id": "_o3Pfbrofeyx"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}